{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab09bb3",
   "metadata": {},
   "source": [
    "All necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27c5c9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import llama_cpp\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a79533",
   "metadata": {},
   "source": [
    "Path of pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2010b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"D:\\\\personalCode\\\\RAG-Toolkit\\\\documents\\\\sample.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b36529c",
   "metadata": {},
   "source": [
    "PDF text reading function, text splitting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1753748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_text(path):\n",
    "    doc=pymupdf.open(path)\n",
    "    full_text=\"\"\n",
    "    for page in doc:\n",
    "        full_text+=page.get_text()\n",
    "    return full_text, doc\n",
    "\n",
    "def split_into_chunks(text, chunk_size=500, overlap=50):\n",
    "    chunks=[]\n",
    "    for i in range(0, len(text), chunk_size-overlap):\n",
    "        chunks.append(text[i:i+chunk_size])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96185681",
   "metadata": {},
   "outputs": [],
   "source": [
    "text, doc=read_pdf_text(path)\n",
    "chunks=split_into_chunks(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478da314",
   "metadata": {},
   "source": [
    "Embedding and storage of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf9803b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder=SentenceTransformer('all-MiniLM-L6-v2')\n",
    "vectors=embedder.encode(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d05f5f",
   "metadata": {},
   "source": [
    "Model being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e509a7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes\n",
      "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4060 Laptop GPU) - 7099 MiB free\n",
      "llama_model_loader: loaded meta data with 81 key-value pairs and 255 tensors from D:\\personalCode\\RAG-Toolkit\\models\\Dolphin3.0-Llama3.2-3B-Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Dolphin 3.0 Llama 3.2 3B\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Cognitivecomputations\n",
      "llama_model_loader: - kv   4:                           general.basename str              = dolphin-3.0-Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.2\n",
      "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   8:                  general.base_model.0.name str              = Llama 3.2 3B\n",
      "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Meta Llama\n",
      "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/meta-llama/Lla...\n",
      "llama_model_loader: - kv  11:                      general.dataset.count u32              = 13\n",
      "llama_model_loader: - kv  12:                     general.dataset.0.name str              = Opc Sft Stage1\n",
      "llama_model_loader: - kv  13:             general.dataset.0.organization str              = OpenCoder LLM\n",
      "llama_model_loader: - kv  14:                 general.dataset.0.repo_url str              = https://huggingface.co/OpenCoder-LLM/...\n",
      "llama_model_loader: - kv  15:                     general.dataset.1.name str              = Opc Sft Stage2\n",
      "llama_model_loader: - kv  16:             general.dataset.1.organization str              = OpenCoder LLM\n",
      "llama_model_loader: - kv  17:                 general.dataset.1.repo_url str              = https://huggingface.co/OpenCoder-LLM/...\n",
      "llama_model_loader: - kv  18:                     general.dataset.2.name str              = Orca Agentinstruct 1M v1\n",
      "llama_model_loader: - kv  19:                  general.dataset.2.version str              = v1\n",
      "llama_model_loader: - kv  20:             general.dataset.2.organization str              = Microsoft\n",
      "llama_model_loader: - kv  21:                 general.dataset.2.repo_url str              = https://huggingface.co/microsoft/orca...\n",
      "llama_model_loader: - kv  22:                     general.dataset.3.name str              = Orca Math Word Problems 200k\n",
      "llama_model_loader: - kv  23:             general.dataset.3.organization str              = Microsoft\n",
      "llama_model_loader: - kv  24:                 general.dataset.3.repo_url str              = https://huggingface.co/microsoft/orca...\n",
      "llama_model_loader: - kv  25:                     general.dataset.4.name str              = Hermes Function Calling v1\n",
      "llama_model_loader: - kv  26:                  general.dataset.4.version str              = v1\n",
      "llama_model_loader: - kv  27:             general.dataset.4.organization str              = NousResearch\n",
      "llama_model_loader: - kv  28:                 general.dataset.4.repo_url str              = https://huggingface.co/NousResearch/h...\n",
      "llama_model_loader: - kv  29:                     general.dataset.5.name str              = NuminaMath CoT\n",
      "llama_model_loader: - kv  30:             general.dataset.5.organization str              = AI MO\n",
      "llama_model_loader: - kv  31:                 general.dataset.5.repo_url str              = https://huggingface.co/AI-MO/NuminaMa...\n",
      "llama_model_loader: - kv  32:                     general.dataset.6.name str              = NuminaMath TIR\n",
      "llama_model_loader: - kv  33:             general.dataset.6.organization str              = AI MO\n",
      "llama_model_loader: - kv  34:                 general.dataset.6.repo_url str              = https://huggingface.co/AI-MO/NuminaMa...\n",
      "llama_model_loader: - kv  35:                     general.dataset.7.name str              = Tulu 3 Sft Mixture\n",
      "llama_model_loader: - kv  36:             general.dataset.7.organization str              = Allenai\n",
      "llama_model_loader: - kv  37:                 general.dataset.7.repo_url str              = https://huggingface.co/allenai/tulu-3...\n",
      "llama_model_loader: - kv  38:                     general.dataset.8.name str              = Dolphin Coder\n",
      "llama_model_loader: - kv  39:             general.dataset.8.organization str              = Cognitivecomputations\n",
      "llama_model_loader: - kv  40:                 general.dataset.8.repo_url str              = https://huggingface.co/cognitivecompu...\n",
      "llama_model_loader: - kv  41:                     general.dataset.9.name str              = Smoltalk\n",
      "llama_model_loader: - kv  42:             general.dataset.9.organization str              = HuggingFaceTB\n",
      "llama_model_loader: - kv  43:                 general.dataset.9.repo_url str              = https://huggingface.co/HuggingFaceTB/...\n",
      "llama_model_loader: - kv  44:                    general.dataset.10.name str              = Samantha Data\n",
      "llama_model_loader: - kv  45:            general.dataset.10.organization str              = Cognitivecomputations\n",
      "llama_model_loader: - kv  46:                general.dataset.10.repo_url str              = https://huggingface.co/cognitivecompu...\n",
      "llama_model_loader: - kv  47:                    general.dataset.11.name str              = CodeFeedback Filtered Instruction\n",
      "llama_model_loader: - kv  48:            general.dataset.11.organization str              = M A P\n",
      "llama_model_loader: - kv  49:                general.dataset.11.repo_url str              = https://huggingface.co/m-a-p/CodeFeed...\n",
      "llama_model_loader: - kv  50:                    general.dataset.12.name str              = Code Feedback\n",
      "llama_model_loader: - kv  51:            general.dataset.12.organization str              = M A P\n",
      "llama_model_loader: - kv  52:                general.dataset.12.repo_url str              = https://huggingface.co/m-a-p/Code-Fee...\n",
      "llama_model_loader: - kv  53:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  54:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv  55:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  56:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  57:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  58:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  59:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  60:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  61:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  62:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  63:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  64:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  65:                           llama.vocab_size u32              = 128258\n",
      "llama_model_loader: - kv  66:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  67:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  68:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  69:                      tokenizer.ggml.tokens arr[str,128258]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  70:                  tokenizer.ggml.token_type arr[i32,128258]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  71:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  72:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  73:                tokenizer.ggml.eos_token_id u32              = 128256\n",
      "llama_model_loader: - kv  74:            tokenizer.ggml.padding_token_id u32              = 128001\n",
      "llama_model_loader: - kv  75:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  76:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  77:                      quantize.imatrix.file str              = /models_out/Dolphin3.0-Llama3.2-3B-GG...\n",
      "llama_model_loader: - kv  78:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  79:             quantize.imatrix.entries_count i32              = 196\n",
      "llama_model_loader: - kv  80:              quantize.imatrix.chunks_count i32              = 125\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type q5_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q5_K - Medium\n",
      "print_info: file size   = 2.16 GiB (5.76 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: control token: 128257 '<|im_start|>' is not marked as EOG\n",
      "load: special tokens cache size = 258\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 3072\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 24\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 3\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.21 B\n",
      "print_info: general.name     = Dolphin 3.0 Llama 3.2 3B\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128258\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128256 '<|im_end|>'\n",
      "print_info: EOT token        = 128256 '<|im_end|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: PAD token        = 128001 '<|end_of_text|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: EOG token        = 128256 '<|im_end|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\n",
      "load_tensors: offloading 28 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 29/29 layers to GPU\n",
      "load_tensors:        CUDA0 model buffer size =  2207.11 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   308.24 MiB\n",
      ".............................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 8192\n",
      "llama_context: n_ctx_per_seq = 8192\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "create_memory: n_ctx = 8192 (padded)\n",
      "llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   1: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   2: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   3: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   4: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   5: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   6: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   7: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   8: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   9: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  10: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  11: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  12: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  13: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  14: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  15: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  16: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  17: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  18: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  19: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  20: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  21: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  22: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  23: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  24: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  25: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  26: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  27: dev = CUDA0\n",
      "llama_kv_cache_unified:      CUDA0 KV buffer size =   896.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 2\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:      CUDA0 compute buffer size =   424.00 MiB\n",
      "llama_context:  CUDA_Host compute buffer size =    22.01 MiB\n",
      "llama_context: graph nodes  = 958\n",
      "llama_context: graph splits = 2\n",
      "CUDA : ARCHS = 890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.dataset.10.repo_url': 'https://huggingface.co/cognitivecomputations/samantha-data', 'general.name': 'Dolphin 3.0 Llama 3.2 3B', 'general.dataset.9.repo_url': 'https://huggingface.co/HuggingFaceTB/smoltalk', 'general.dataset.8.name': 'Dolphin Coder', 'general.architecture': 'llama', 'general.type': 'model', 'llama.context_length': '131072', 'general.organization': 'Cognitivecomputations', 'general.basename': 'dolphin-3.0-Llama-3.2', 'general.dataset.0.repo_url': 'https://huggingface.co/OpenCoder-LLM/opc-sft-stage1', 'general.dataset.12.name': 'Code Feedback', 'general.size_label': '3B', 'general.license': 'llama3.2', 'general.dataset.9.organization': 'HuggingFaceTB', 'general.base_model.count': '1', 'general.base_model.0.name': 'Llama 3.2 3B', 'general.base_model.0.organization': 'Meta Llama', 'llama.attention.value_length': '128', 'general.base_model.0.repo_url': 'https://huggingface.co/meta-llama/Llama-3.2-3B', 'general.dataset.2.name': 'Orca Agentinstruct 1M v1', 'general.dataset.11.organization': 'M A P', 'general.dataset.count': '13', 'llama.attention.key_length': '128', 'general.dataset.0.name': 'Opc Sft Stage1', 'llama.rope.freq_base': '500000.000000', 'general.dataset.5.repo_url': 'https://huggingface.co/AI-MO/NuminaMath-CoT', 'general.dataset.4.name': 'Hermes Function Calling v1', 'general.dataset.0.organization': 'OpenCoder LLM', 'general.dataset.1.name': 'Opc Sft Stage2', 'general.dataset.1.organization': 'OpenCoder LLM', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.dataset.1.repo_url': 'https://huggingface.co/OpenCoder-LLM/opc-sft-stage2', 'general.dataset.2.version': 'v1', 'general.dataset.2.organization': 'Microsoft', 'general.dataset.12.organization': 'M A P', 'general.dataset.2.repo_url': 'https://huggingface.co/microsoft/orca-agentinstruct-1M-v1', 'general.dataset.3.name': 'Orca Math Word Problems 200k', 'general.dataset.3.organization': 'Microsoft', 'general.dataset.3.repo_url': 'https://huggingface.co/microsoft/orca-math-word-problems-200k', 'general.dataset.10.organization': 'Cognitivecomputations', 'general.dataset.4.version': 'v1', 'general.dataset.11.name': 'CodeFeedback Filtered Instruction', 'general.dataset.4.organization': 'NousResearch', 'general.dataset.5.name': 'NuminaMath CoT', 'general.dataset.4.repo_url': 'https://huggingface.co/NousResearch/hermes-function-calling-v1', 'general.dataset.5.organization': 'AI MO', 'general.dataset.6.name': 'NuminaMath TIR', 'general.dataset.6.organization': 'AI MO', 'general.dataset.6.repo_url': 'https://huggingface.co/AI-MO/NuminaMath-TIR', 'general.dataset.7.name': 'Tulu 3 Sft Mixture', 'llama.feed_forward_length': '8192', 'general.dataset.7.organization': 'Allenai', 'general.dataset.7.repo_url': 'https://huggingface.co/allenai/tulu-3-sft-mixture', 'general.dataset.8.organization': 'Cognitivecomputations', 'general.dataset.8.repo_url': 'https://huggingface.co/cognitivecomputations/dolphin-coder', 'general.dataset.9.name': 'Smoltalk', 'general.dataset.10.name': 'Samantha Data', 'llama.block_count': '28', 'general.dataset.11.repo_url': 'https://huggingface.co/m-a-p/CodeFeedback-Filtered-Instruction', 'general.file_type': '17', 'llama.attention.head_count_kv': '8', 'tokenizer.ggml.eos_token_id': '128256', 'general.dataset.12.repo_url': 'https://huggingface.co/m-a-p/Code-Feedback', 'llama.embedding_length': '3072', 'llama.attention.head_count': '24', 'llama.vocab_size': '128258', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.padding_token_id': '128001', 'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'general.quantization_version': '2', 'quantize.imatrix.file': '/models_out/Dolphin3.0-Llama3.2-3B-GGUF/Dolphin3.0-Llama3.2-3B.imatrix', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'quantize.imatrix.entries_count': '196', 'quantize.imatrix.chunks_count': '125'}\n",
      "Available chat formats from metadata: chat_template.default\n"
     ]
    }
   ],
   "source": [
    "model_path_gguf=\"D:\\\\personalCode\\\\RAG-Toolkit\\models\\\\Dolphin3.0-Llama3.2-3B-Q5_K_M.gguf\"\n",
    "model=llama_cpp.Llama(model_path=model_path_gguf, chat_format=\"llama-2\", n_ctx=8192, n_gpu_layers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c891863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n"
     ]
    }
   ],
   "source": [
    "print(model.context_params.n_ctx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d740b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(vectors, num_clusters):\n",
    "    \"\"\"\n",
    "    input: embeddings from given pdf text as vectors\n",
    "    output: clusters of similar vectors\n",
    "    \"\"\"\n",
    "    k=num_clusters\n",
    "    kmeans=KMeans(n_clusters=k, random_state=42).fit(vectors)\n",
    "    labels=kmeans.labels_\n",
    "    closest_indices=[]\n",
    "\n",
    "    for i in range(num_clusters):\n",
    "        distances=np.linalg.norm(vectors-kmeans.cluster_centers_[i], axis=1)\n",
    "\n",
    "        closest_index=np.argmin(distances)\n",
    "\n",
    "        closest_indices.append(closest_index)\n",
    "\n",
    "    selected_indices=sorted(closest_indices)\n",
    "    return selected_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fee074f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, 123, 189, 233, 282, 390, 506, 547, 551, 657]\n"
     ]
    }
   ],
   "source": [
    "selected_indices=clustering(vectors, 10)\n",
    "print(selected_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "447afe68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "excuses not to buckle down and reach your optimum level of fitness. The ironic thing is \n",
      "that people often feel they have to put themselves through far harsher and lengthy routines \n",
      "in the gym than the more effective bodyweight programs explained in this book.  \n",
      "I’ve visited hundreds of gyms in my career. And the proof is in the pudding. I look at \n",
      "the people there. Then I look at my SpecOps troops. The difference is night and day. And \n",
      "you can achieve this difference with an amazingly small sa\n"
     ]
    }
   ],
   "source": [
    "print(chunks[233])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6e8af5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 2 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     540.71 ms\n",
      "llama_perf_context_print: prompt eval time =     293.00 ms /   195 tokens (    1.50 ms per token,   665.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1131.93 ms /    86 runs   (   13.16 ms per token,    75.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1488.76 ms /   281 tokens\n",
      "Llama.generate: 73 prefix-match hit, remaining 102 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The text describes 9 weeks of intense training in a challenging underwater environment. The trainees are required to commit fully, tying three different knots perfectly underwater. The instructors aim to make the trainees quit, but the full commitment to the training and tasks leads to success. The training environment is described as challenging, and the trainees learn to commit, stay down, and overcome the initial discomfort. Success is achieved through full commitment.\n",
      "42\n",
      "Summary for chunk42 is ready, 1 indices covered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     540.71 ms\n",
      "llama_perf_context_print: prompt eval time =      34.08 ms /   102 tokens (    0.33 ms per token,  2992.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1305.77 ms /   103 runs   (   12.68 ms per token,    78.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1415.47 ms /   205 tokens\n",
      "Llama.generate: 73 prefix-match hit, remaining 113 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The text focuses on the resting metabolic rate (RMR) which is crucial for maintaining a lean body. RMR is influenced significantly by body composition, particularly the presence of muscle. Muscle is the most effective calorie burner. The text emphasizes the importance of making positive changes in body composition, specifically gaining muscle, rather than just focusing on weight loss. Losing muscle weight is detrimental and counterproductive to achieving a lean body. The concept of calories in vs. calories out is discussed in relation to body composition changes.```\n",
      "\n",
      "\n",
      "123\n",
      "Summary for chunk123 is ready, 2 indices covered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     540.71 ms\n",
      "llama_perf_context_print: prompt eval time =      36.16 ms /   113 tokens (    0.32 ms per token,  3124.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =     779.90 ms /    62 runs   (   12.58 ms per token,    79.50 tokens per second)\n",
      "llama_perf_context_print:       total time =     863.00 ms /   175 tokens\n",
      "Llama.generate: 72 prefix-match hit, remaining 115 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The text provides information about the importance of the post workout meal, which consists of 30-50 grams of lean protein and 30-50 grams of high glycemic index carbohydrates. The lean protein is essential to ensure that the body absorbs the nutrients properly and efficiently, as fat slows down this absorption process.\n",
      "189\n",
      "Summary for chunk189 is ready, 3 indices covered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     540.71 ms\n",
      "llama_perf_context_print: prompt eval time =      36.34 ms /   115 tokens (    0.32 ms per token,  3164.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =     682.95 ms /    54 runs   (   12.65 ms per token,    79.07 tokens per second)\n",
      "llama_perf_context_print:       total time =     756.68 ms /   169 tokens\n",
      "Llama.generate: 72 prefix-match hit, remaining 146 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The text discusses how people often give excuses for not reaching their optimal fitness level. It highlights that more effective bodyweight programs can be found in a book than harsher gym routines. The author shares their career experience visiting gyms and compares it to their SpecOps troops.\n",
      "233\n",
      "Summary for chunk233 is ready, 4 indices covered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     540.71 ms\n",
      "llama_perf_context_print: prompt eval time =      46.38 ms /   146 tokens (    0.32 ms per token,  3148.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1005.13 ms /    79 runs   (   12.72 ms per token,    78.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1108.47 ms /   225 tokens\n",
      "Llama.generate: 73 prefix-match hit, remaining 141 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The text discusses the use of resistance bands in a fitness program. The bands are divided into four sections: Push, Pull, Core, and Legs. Each muscle group needs to be worked once a week. The standard gym training regimen can also be used. The muscle groups are broken down into specific exercises such as shoulders, triceps, chest, lats, and biceps and forearms.\n",
      "282\n",
      "Summary for chunk282 is ready, 5 indices covered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     540.71 ms\n",
      "llama_perf_context_print: prompt eval time =      45.82 ms /   141 tokens (    0.32 ms per token,  3077.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =     801.01 ms /    63 runs   (   12.71 ms per token,    78.65 tokens per second)\n",
      "llama_perf_context_print:       total time =     891.36 ms /   204 tokens\n",
      "Llama.generate: 73 prefix-match hit, remaining 212 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The text describes a modified push-up exercise called \"Press shoulders, triceps (2-4)\" where you perform the exercise with shoulder-width apart hands, similar to a Chinese Push Up. The exercise can be increased in difficulty by placing your hands on a raised surface, allowing your head to come below your hands.\n",
      "390\n",
      "Summary for chunk390 is ready, 6 indices covered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     540.71 ms\n",
      "llama_perf_context_print: prompt eval time =      59.41 ms /   212 tokens (    0.28 ms per token,  3568.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =     351.39 ms /    27 runs   (   13.01 ms per token,    76.84 tokens per second)\n",
      "llama_perf_context_print:       total time =     429.82 ms /   239 tokens\n",
      "Llama.generate: 73 prefix-match hit, remaining 263 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sumo Squat - Lift yourself up until your legs are straight again. YOU ARE YOUR OWN GYM - 104.\n",
      "    ```\n",
      "\n",
      "506\n",
      "Summary for chunk506 is ready, 7 indices covered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     540.71 ms\n",
      "llama_perf_context_print: prompt eval time =      75.14 ms /   263 tokens (    0.29 ms per token,  3499.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1969.64 ms /   149 runs   (   13.22 ms per token,    75.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    2160.52 ms /   412 tokens\n",
      "Llama.generate: 73 prefix-match hit, remaining 205 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - The text describes a fitness routine involving explosive movements.\n",
      "     - It also mentions a specific exercise called \"Pistols\" and provides instructions for its performance.\n",
      "     - The text ends with an instruction to \"bring your butt all the way down to the heel of your working foot\" during the exercise.\n",
      "     - Finally, it provides a summary of the fitness routine, suggesting that the reader should \"really ready to kick it up a notch.\"\n",
      "    ```\u000f\n",
      "    ```\n",
      "\n",
      "The given text seems to be about fitness routine or exercise instructions. However, it lacks the context or purpose of such instructions. It primarily focuses on a specific exercise called \"Pistols\" and provides step-by-step instructions for its performance. The text also mentions a fitness\n",
      "547\n",
      "Summary for chunk547 is ready, 8 indices covered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     540.71 ms\n",
      "llama_perf_context_print: prompt eval time =      59.46 ms /   205 tokens (    0.29 ms per token,  3447.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =     635.11 ms /    49 runs   (   12.96 ms per token,    77.15 tokens per second)\n",
      "llama_perf_context_print:       total time =     730.73 ms /   254 tokens\n",
      "Llama.generate: 73 prefix-match hit, remaining 174 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Squats target thighs, hamstrings, and glutes.\n",
      "     - Mimics leg extensions with more muscle involvement.\n",
      "     - Additional resistance options available (e.g., heavy object or backpack).\n",
      "     - One-leg squats can improve balance.\n",
      "551\n",
      "Summary for chunk551 is ready, 9 indices covered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     540.71 ms\n",
      "llama_perf_context_print: prompt eval time =      52.21 ms /   174 tokens (    0.30 ms per token,  3332.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =     913.96 ms /    71 runs   (   12.87 ms per token,    77.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1017.21 ms /   245 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This week's workout focuses on core stability and functional movements. Exercises include 1-legged hip extensions, supermans, push-ups with feet elevated, assisted dips, alternating 1-legged RDLs on a pillow, box jumps with reverse grip, V-ups, and Russian twists. The undulating block schedule ensures variety and intensity are maintained throughout the week.\n",
      "657\n",
      "Summary for chunk657 is ready, 10 indices covered\n"
     ]
    }
   ],
   "source": [
    "summary_list=[]\n",
    "j=0\n",
    "for i in selected_indices:\n",
    "    section=chunks[i]\n",
    "    map_prompt=f\"\"\"\n",
    "    Act as a concise summariser.\n",
    "    Summarise the given text into 2-3 lines, no more. Ensure you completely cover the content of the text. This text will be enclosed in triple backticks (```)\n",
    "    The output should be the summary of the user supplied text.\n",
    "    Be concise and precise in your behaviour.\n",
    "\n",
    "    ```{section}```\n",
    "    SUMMARY: \n",
    "    \"\"\"\n",
    "    temp=0.7\n",
    "    max_tokens=150\n",
    "\n",
    "    response=model.create_completion(\n",
    "    prompt=map_prompt,\n",
    "    temperature=temp,\n",
    "    max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    summary=response['choices'][0]['text']\n",
    "    summary=summary.replace(\"[/INST]\", \"\")\n",
    "    print(summary)\n",
    "    print(i)\n",
    "    summary_list.append(summary)\n",
    "    j=j+1\n",
    "    print(f\"Summary for chunk{i} is ready, {j} indices covered\")\n",
    "\n",
    "summaries=\"\\n\".join(summary_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b4ff4fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The text describes 9 weeks of intense training in a challenging underwater environment. The trainees are required to commit fully, tying three different knots perfectly underwater. The instructors aim to make the trainees quit, but the full commitment to the training and tasks leads to success. The training environment is described as challenging, and the trainees learn to commit, stay down, and overcome the initial discomfort. Success is achieved through full commitment.\n",
      " The text focuses on the resting metabolic rate (RMR) which is crucial for maintaining a lean body. RMR is influenced significantly by body composition, particularly the presence of muscle. Muscle is the most effective calorie burner. The text emphasizes the importance of making positive changes in body composition, specifically gaining muscle, rather than just focusing on weight loss. Losing muscle weight is detrimental and counterproductive to achieving a lean body. The concept of calories in vs. calories out is discussed in relation to body composition changes.```\n",
      "\n",
      "\n",
      " The text provides information about the importance of the post workout meal, which consists of 30-50 grams of lean protein and 30-50 grams of high glycemic index carbohydrates. The lean protein is essential to ensure that the body absorbs the nutrients properly and efficiently, as fat slows down this absorption process.\n",
      " The text discusses how people often give excuses for not reaching their optimal fitness level. It highlights that more effective bodyweight programs can be found in a book than harsher gym routines. The author shares their career experience visiting gyms and compares it to their SpecOps troops.\n",
      " The text discusses the use of resistance bands in a fitness program. The bands are divided into four sections: Push, Pull, Core, and Legs. Each muscle group needs to be worked once a week. The standard gym training regimen can also be used. The muscle groups are broken down into specific exercises such as shoulders, triceps, chest, lats, and biceps and forearms.\n",
      " The text describes a modified push-up exercise called \"Press shoulders, triceps (2-4)\" where you perform the exercise with shoulder-width apart hands, similar to a Chinese Push Up. The exercise can be increased in difficulty by placing your hands on a raised surface, allowing your head to come below your hands.\n",
      " Sumo Squat - Lift yourself up until your legs are straight again. YOU ARE YOUR OWN GYM - 104.\n",
      "    ```\n",
      "\n",
      " - The text describes a fitness routine involving explosive movements.\n",
      "     - It also mentions a specific exercise called \"Pistols\" and provides instructions for its performance.\n",
      "     - The text ends with an instruction to \"bring your butt all the way down to the heel of your working foot\" during the exercise.\n",
      "     - Finally, it provides a summary of the fitness routine, suggesting that the reader should \"really ready to kick it up a notch.\"\n",
      "    ```\u000f\n",
      "    ```\n",
      "\n",
      "The given text seems to be about fitness routine or exercise instructions. However, it lacks the context or purpose of such instructions. It primarily focuses on a specific exercise called \"Pistols\" and provides step-by-step instructions for its performance. The text also mentions a fitness\n",
      " - Squats target thighs, hamstrings, and glutes.\n",
      "     - Mimics leg extensions with more muscle involvement.\n",
      "     - Additional resistance options available (e.g., heavy object or backpack).\n",
      "     - One-leg squats can improve balance.\n",
      " This week's workout focuses on core stability and functional movements. Exercises include 1-legged hip extensions, supermans, push-ups with feet elevated, assisted dips, alternating 1-legged RDLs on a pillow, box jumps with reverse grip, V-ups, and Russian twists. The undulating block schedule ensures variety and intensity are maintained throughout the week.\n"
     ]
    }
   ],
   "source": [
    "print(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6de4bad",
   "metadata": {},
   "source": [
    "User input and search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2d69f389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 2 prefix-match hit, remaining 906 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     540.71 ms\n",
      "llama_perf_context_print: prompt eval time =     601.47 ms /   906 tokens (    0.66 ms per token,  1506.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1405.57 ms /   105 runs   (   13.39 ms per token,    74.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    2083.15 ms /  1011 tokens\n"
     ]
    }
   ],
   "source": [
    "final_prompt = f\"\"\"\n",
    "You are a precise and concise summariser.\n",
    "You will be given a series of summaries from a book. The summaries will be enclosed in triple backticks (```).\n",
    "Your task is to write a verbose summary of what was covered in the book.\n",
    "\n",
    "The output should be a detailed and coherent summary that captures all the key information present in the provided summaries. Combine each summary into one whole summary \n",
    "The goal is to help a reader understand the entire content of the book from this single collated summary. \n",
    "\n",
    "Do not add any external information. Base your answer only on what is provided. Ensure it is a single stream of text, and not split up. Combine parts to form a bigger whole.\n",
    "Capture the sentiment of the book.\n",
    "\n",
    "```{summaries}```\n",
    "\n",
    "SUMMARY:\n",
    "Here is the detailed summary of the book:\n",
    "\"\"\"\n",
    "\n",
    "temp=0.7\n",
    "max_tokens=3000\n",
    "\n",
    "response=model.create_completion(\n",
    "    prompt=final_prompt,\n",
    "    temperature=temp,\n",
    "    max_tokens=max_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f3c9d423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-3c2d24af-33b1-47dd-a81b-26558e290c70', 'object': 'text_completion', 'created': 1747928271, 'model': 'D:\\\\personalCode\\\\RAG-Toolkit\\\\models\\\\Dolphin3.0-Llama3.2-3B-Q5_K_M.gguf', 'choices': [{'text': 'The book focuses on fitness routines and exercise instructions, particularly explosive movements and core stability. It provides step-by-step instructions for specific exercises such as \"Pistols\" and a modified push-up exercise called \"Press shoulders, triceps (2-4)\". The book emphasizes the importance of muscle involvement and provides various resistance options for squats. It also highlights the role of bodyweight programs in achieving fitness goals. The text describes fitness routines that mimic military training and encourages readers to \"really ready to kick it up a notch.\"', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 908, 'completion_tokens': 105, 'total_tokens': 1013}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fb3713d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book focuses on fitness routines and exercise instructions, particularly explosive movements and core stability. It provides step-by-step instructions for specific exercises such as \"Pistols\" and a modified push-up exercise called \"Press shoulders, triceps (2-4)\". The book emphasizes the importance of muscle involvement and provides various resistance options for squats. It also highlights the role of bodyweight programs in achieving fitness goals. The text describes fitness routines that mimic military training and encourages readers to \"really ready to kick it up a notch.\"\n"
     ]
    }
   ],
   "source": [
    "assistant_reply=response['choices'][0]['text']\n",
    "assistant_reply=assistant_reply.replace(\"[/INST]\", \"\")\n",
    "print(assistant_reply)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
